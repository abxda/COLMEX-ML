{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAPO2YmcSKZAJk1vloiiXA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abxda/COLMEX-ML/blob/main/Semana_06_COLMEX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas geopandas seaborn tqdm requests loguru geohexgrid --quiet"
      ],
      "metadata": {
        "id": "wFlI5vGcctZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Script para integrar datos censales y geográficos de 32 estados de México en DuckDB,\n",
        "procesando archivos CSV y shapefiles para generar:\n",
        "  - La tabla 'censo_geo_int' con datos censales, con campos numéricos convertidos a INTEGER.\n",
        "  - La tabla 'hex_grid_1km_4326' que contiene una grilla hexagonal calculada con geohexgrid.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "6f2Z_lqgc_2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "from zipfile import ZipFile\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import duckdb\n",
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import chardet\n",
        "import geohexgrid as ghg\n",
        "from shapely import wkb\n"
      ],
      "metadata": {
        "id": "KPKh7aOEdCeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 1. Funciones Base\n",
        "# --------------------------------------------------\n",
        "\n",
        "def download(url, directory):\n",
        "    \"\"\"Descarga un archivo desde la URL indicada en 'directory' (salta si ya existe).\"\"\"\n",
        "    filename = url.split('/')[-1]\n",
        "    filepath = os.path.join(directory, filename)\n",
        "    if os.path.exists(filepath):\n",
        "        print(f\"El archivo {filename} ya existe, no se descarga de nuevo.\")\n",
        "        return filepath\n",
        "    print(f\"Descargando {filename} ...\")\n",
        "    time.sleep(1)\n",
        "    r = requests.get(url, stream=True)\n",
        "    total_size = int(r.headers.get('content-length', 0))\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "    with open(filepath, 'wb') as f, tqdm(\n",
        "        total=total_size/1024 if total_size else None, unit='KB', desc=filename\n",
        "    ) as pbar:\n",
        "        for data in r.iter_content(1024):\n",
        "            f.write(data)\n",
        "            pbar.update(len(data)/1024)\n",
        "    print(f\"Descarga completada: {filename}\\n\")\n",
        "    return filepath\n",
        "\n",
        "def extract_shapefile(geo_zip_list, zip_dir, shp_dir, shape_type):\n",
        "    \"\"\"\n",
        "    Extrae los componentes esenciales del shapefile de cada ZIP en geo_zip_list.\n",
        "    Se asume que dentro de cada ZIP los archivos siguen la nomenclatura:\n",
        "      conjunto_de_datos/{CODIGO}{shape_type}.{ext}\n",
        "    \"\"\"\n",
        "    os.makedirs(shp_dir, exist_ok=True)\n",
        "    for estado_zip in geo_zip_list:\n",
        "        cod = estado_zip.split('_')[0]\n",
        "        zip_path = os.path.join(zip_dir, estado_zip)\n",
        "        shp_files = [\n",
        "            f'conjunto_de_datos/{cod}{shape_type}.shp',\n",
        "            f'conjunto_de_datos/{cod}{shape_type}.cpg',\n",
        "            f'conjunto_de_datos/{cod}{shape_type}.dbf',\n",
        "            f'conjunto_de_datos/{cod}{shape_type}.prj',\n",
        "            f'conjunto_de_datos/{cod}{shape_type}.shx'\n",
        "        ]\n",
        "        # Verifica si ya están extraídos\n",
        "        if all(os.path.exists(os.path.join(shp_dir, f)) for f in shp_files):\n",
        "            print(f\"Archivos para {cod} ya extraídos en {shp_dir}.\")\n",
        "            continue\n",
        "        with ZipFile(zip_path, 'r') as zip_ref:\n",
        "            for f in shp_files:\n",
        "                try:\n",
        "                    zip_ref.extract(f, shp_dir)\n",
        "                except KeyError:\n",
        "                    if f.endswith('.cpg'):\n",
        "                        with open(os.path.join(shp_dir, f), 'w') as out_file:\n",
        "                            out_file.write(\"ISO 88591\")\n",
        "                    else:\n",
        "                        print(f\"¡ERROR! Archivo esencial '{f}' no encontrado en {estado_zip}.\")\n",
        "        print(f\"Shapefile extraído para {cod} en {shp_dir}.\\n\")\n",
        "\n",
        "def convert_to_geoparquet(shp_dir, cod, shape_type=\"m\"):\n",
        "    \"\"\"\n",
        "    Convierte el shapefile de un estado a GeoParquet.\n",
        "    Se asume que el shapefile se encuentra en:\n",
        "      {shp_dir}/conjunto_de_datos/{cod}{shape_type}.shp\n",
        "    \"\"\"\n",
        "    shp_path = os.path.join(shp_dir, \"conjunto_de_datos\", f\"{cod}{shape_type}.shp\")\n",
        "    parquet_path = os.path.join(shp_dir, \"conjunto_de_datos\", f\"{cod}{shape_type}.geoparquet\")\n",
        "    if os.path.exists(parquet_path):\n",
        "        print(f\"GeoParquet para {cod} ya existe.\")\n",
        "    else:\n",
        "        print(f\"Convirtiendo {shp_path} a GeoParquet...\")\n",
        "        try:\n",
        "            gdf = gpd.read_file(shp_path, encoding='ISO-8859-1')\n",
        "            if gdf.crs is None:\n",
        "                gdf.set_crs(\"EPSG:6372\", inplace=True)\n",
        "            gdf = gdf.to_crs(\"EPSG:4326\")\n",
        "            gdf.to_parquet(parquet_path, index=False)\n",
        "            print(f\"  GeoParquet guardado en: {parquet_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error al convertir {shp_path} a GeoParquet: {e}\")\n",
        "    return parquet_path"
      ],
      "metadata": {
        "id": "nSlPZhazdFyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --------------------------------------------------\n",
        "# 2. Configuración de Directorios y Listas\n",
        "# --------------------------------------------------\n",
        "\n",
        "# Listas de archivos (shapefiles) y números de estados\n",
        "#estados_geo = [\n",
        "#    \"01_aguascalientes.zip\", \"02_bajacalifornia.zip\", \"03_bajacaliforniasur.zip\",\n",
        "#    \"04_campeche.zip\", \"05_coahuiladezaragoza.zip\", \"06_colima.zip\", \"07_chiapas.zip\",\n",
        "#    \"08_chihuahua.zip\", \"09_ciudaddemexico.zip\", \"10_durango.zip\", \"11_guanajuato.zip\",\n",
        "#    \"12_guerrero.zip\", \"13_hidalgo.zip\", \"14_jalisco.zip\", \"15_mexico.zip\",\n",
        "#    \"16_michoacandeocampo.zip\", \"17_morelos.zip\", \"18_nayarit.zip\", \"19_nuevoleon.zip\",\n",
        "#    \"20_oaxaca.zip\", \"21_puebla.zip\", \"22_queretaro.zip\", \"23_quintanaroo.zip\",\n",
        "#    \"24_sanluispotosi.zip\", \"25_sinaloa.zip\", \"26_sonora.zip\", \"27_tabasco.zip\",\n",
        "#    \"28_tamaulipas.zip\", \"29_tlaxcala.zip\", \"30_veracruzignaciodelallave.zip\",\n",
        "#    \"31_yucatan.zip\", \"32_zacatecas.zip\"\n",
        "#]\n",
        "#estados_num = list(range(1, 33))\n",
        "\n",
        "\n",
        "estados_geo = [\"09_ciudaddemexico.zip\",\"13_hidalgo.zip\", \"15_mexico.zip\"]\n",
        "estados_num = [9,13,15]\n",
        "\n",
        "\n",
        "# Directorios de trabajo\n",
        "base_dir = \"./inegi\"\n",
        "download_dir_csv = os.path.join(base_dir, \"ccpvagebmza\")\n",
        "csv_dir = os.path.join(download_dir_csv, \"csv\")\n",
        "parquet_csv_dir = os.path.join(download_dir_csv, \"parquet\")\n",
        "mgc_dir = os.path.join(base_dir, \"mgccpv\")\n",
        "\n",
        "# Directorios para GeoParquet unificados\n",
        "geo_parquet_unificado = os.path.join(mgc_dir, \"geoparquet_unificados\")\n",
        "\n",
        "# Limpieza y creación de directorios\n",
        "for d in [download_dir_csv, mgc_dir]:\n",
        "    if os.path.exists(d):\n",
        "        shutil.rmtree(d)\n",
        "os.makedirs(csv_dir, exist_ok=True)\n",
        "os.makedirs(parquet_csv_dir, exist_ok=True)\n",
        "os.makedirs(mgc_dir, exist_ok=True)\n",
        "os.makedirs(geo_parquet_unificado, exist_ok=True)"
      ],
      "metadata": {
        "id": "dUuNwAufdF1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------\n",
        "# 3. Descarga, Extracción y Conversión de Datos\n",
        "# --------------------------------------------------\n",
        "\n",
        "csv_files = []       # Rutas de CSV extraídos\n",
        "parquet_geo_files = []   # Rutas de GeoParquet\n",
        "\n",
        "# URLs base\n",
        "base_url_csv = \"https://www.inegi.org.mx/contenidos/programas/ccpv/2020/datosabiertos/ageb_manzana/\"\n",
        "base_url_shp = \"https://www.inegi.org.mx/contenidos/productos/prod_serv/contenidos/espanol/bvinegi/productos/geografia/marcogeo/889463807469/\"\n",
        "shape_type = \"m\"  # Usado para manzanas\n",
        "\n",
        "for num, geo_zip in zip(estados_num, estados_geo):\n",
        "    estado_str = f\"{num:02d}\"\n",
        "    print(f\"\\nProcesando estado {estado_str} ...\")\n",
        "\n",
        "    # --- Procesar CSV ---\n",
        "    zip_csv_name = f\"ageb_mza_urbana_{estado_str}_cpv2020_csv.zip\"\n",
        "    url_csv = base_url_csv + zip_csv_name\n",
        "    zip_csv_path = download(url_csv, download_dir_csv)\n",
        "\n",
        "    # Ruta interna y externa del CSV\n",
        "    target_folder = f\"ageb_mza_urbana_{estado_str}_cpv2020\"\n",
        "    csv_rel_path = os.path.join(target_folder, \"conjunto_de_datos\", f\"conjunto_de_datos_ageb_urbana_{estado_str}_cpv2020.csv\")\n",
        "    csv_full_folder = os.path.join(csv_dir, target_folder)\n",
        "    os.makedirs(csv_full_folder, exist_ok=True)\n",
        "    csv_file_path = os.path.join(csv_dir, csv_rel_path)\n",
        "    if not os.path.exists(csv_file_path):\n",
        "        with ZipFile(zip_csv_path, 'r') as z:\n",
        "            z.extract(csv_rel_path, csv_dir)\n",
        "        print(f\"CSV extraído para estado {estado_str}.\")\n",
        "    else:\n",
        "        print(f\"CSV para estado {estado_str} ya existe.\")\n",
        "    csv_files.append(csv_file_path)\n",
        "\n",
        "    # --- Procesar Shapefile ---\n",
        "    zip_shp_path = download(base_url_shp + geo_zip, mgc_dir)\n",
        "    shp_state_dir = os.path.join(mgc_dir, \"shp\", estado_str)\n",
        "    extract_shapefile([geo_zip], mgc_dir, shp_state_dir, shape_type)\n",
        "    parquet_geo = convert_to_geoparquet(shp_state_dir, estado_str, shape_type)\n",
        "    parquet_geo_files.append(parquet_geo)\n"
      ],
      "metadata": {
        "id": "SNLsmQaWdJHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------\n",
        "# 4. Conversión de CSV a Parquet (manteniendo estructura de carpetas)\n",
        "# --------------------------------------------------\n",
        "\n",
        "for root, _, files in os.walk(csv_dir):\n",
        "    for file in files:\n",
        "        if file.lower().endswith(\".csv\"):\n",
        "            csv_path = os.path.join(root, file)\n",
        "            relative_path = os.path.relpath(csv_path, csv_dir)\n",
        "            parquet_path = os.path.splitext(os.path.join(parquet_csv_dir, relative_path))[0] + \".parquet\"\n",
        "            os.makedirs(os.path.dirname(parquet_path), exist_ok=True)\n",
        "            print(f\"Convirtiendo {csv_path} a {parquet_path} ...\")\n",
        "            try:\n",
        "                df = pd.read_csv(csv_path, low_memory=False, encoding='utf-8')\n",
        "            except Exception as e:\n",
        "                print(f\"Error leyendo {csv_path} con utf-8: {e}. Se intentará con ISO-8859-1.\")\n",
        "                df = pd.read_csv(csv_path, low_memory=False, encoding='ISO-8859-1')\n",
        "            df.to_parquet(parquet_path, index=False)\n",
        "            print(\"¡Conversión completada!\")\n"
      ],
      "metadata": {
        "id": "auMFzHLVdJJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parquet_path"
      ],
      "metadata": {
        "id": "Ka9erOuKlDPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------\n",
        "# 5. Unificar GeoParquet en un único directorio\n",
        "# --------------------------------------------------\n",
        "\n",
        "for root, _, files in os.walk(os.path.join(mgc_dir, \"shp\")):\n",
        "    for file in files:\n",
        "        if file.lower().endswith(\".geoparquet\"):\n",
        "            src_path = os.path.join(root, file)\n",
        "            dst_path = os.path.join(geo_parquet_unificado, file)\n",
        "            count = 1\n",
        "            while os.path.exists(dst_path):\n",
        "                name, ext = os.path.splitext(file)\n",
        "                dst_path = os.path.join(geo_parquet_unificado, f\"{name}_{count}{ext}\")\n",
        "                count += 1\n",
        "            print(f\"Copiando {src_path} a {dst_path} ...\")\n",
        "            shutil.copy2(src_path, dst_path)\n",
        "print(\"Unificación de GeoParquet completada.\")"
      ],
      "metadata": {
        "id": "lP2y1glpkR2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------\n",
        "# 5.1 Unificar Parquet CSV en un único directorio\n",
        "# --------------------------------------------------\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Directorio fuente donde se generaron los Parquet a partir de los CSV\n",
        "csv_parquet_source = \"/content/inegi/ccpvagebmza/parquet\"\n",
        "\n",
        "# Directorio destino unificado\n",
        "csv_parquet_unificado = \"/content/inegi/ccpvagebmza/parquet_unificados\"\n",
        "os.makedirs(csv_parquet_unificado, exist_ok=True)\n",
        "\n",
        "# Recorrer recursivamente el directorio fuente y copiar los archivos .parquet\n",
        "for root, _, files in os.walk(csv_parquet_source):\n",
        "    for file in files:\n",
        "        if file.lower().endswith(\".parquet\"):\n",
        "            src_path = os.path.join(root, file)\n",
        "            dst_path = os.path.join(csv_parquet_unificado, file)\n",
        "            # Evitar colisiones de nombres añadiendo un sufijo si es necesario\n",
        "            count = 1\n",
        "            while os.path.exists(dst_path):\n",
        "                name, ext = os.path.splitext(file)\n",
        "                dst_path = os.path.join(csv_parquet_unificado, f\"{name}_{count}{ext}\")\n",
        "                count += 1\n",
        "            print(f\"Copiando {src_path} a {dst_path} ...\")\n",
        "            shutil.copy2(src_path, dst_path)\n",
        "\n",
        "print(\"Copia de todos los archivos Parquet CSV en un solo nivel completada.\")\n"
      ],
      "metadata": {
        "id": "5HRYh_lznSrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------\n",
        "# 6. Creación de Tablas en DuckDB\n",
        "# --------------------------------------------------\n",
        "\n",
        "# Definir archivo de base de datos\n",
        "db_file = \"datos_inegi_total_1.duckdb\"\n",
        "if os.path.exists(db_file):\n",
        "    os.remove(db_file)\n",
        "con = duckdb.connect(db_file)\n",
        "con.execute(\"INSTALL spatial;\")\n",
        "con.execute(\"LOAD spatial;\")\n",
        "\n",
        "# Listas de archivos Parquet (CSV y GeoParquet)\n",
        "csv_parquet_files = [\n",
        "    os.path.join(csv_parquet_unificado, f)\n",
        "    for f in os.listdir(csv_parquet_unificado)\n",
        "    if f.lower().endswith(\".parquet\")\n",
        "]\n",
        "geo_parquet_files = [\n",
        "    os.path.join(geo_parquet_unificado, f)\n",
        "    for f in os.listdir(geo_parquet_unificado)\n",
        "    if f.lower().endswith(\".geoparquet\")\n",
        "]\n",
        "\n",
        "# --- Crear tabla censo_all (datos censales) ---\n",
        "con.execute(\"DROP TABLE IF EXISTS censo_all;\")\n",
        "union_queries = [f\"SELECT * FROM read_parquet('{f}')\" for f in csv_parquet_files]\n",
        "query_censo = \" UNION ALL \".join(union_queries)\n",
        "con.execute(f\"CREATE TABLE censo_all AS {query_censo};\")\n",
        "print(\"Tabla 'censo_all' creada con datos censales de todos los estados.\")\n",
        "\n",
        "# --- Crear tabla manzanas (datos geográficos) ---\n",
        "con.execute(\"DROP TABLE IF EXISTS manzanas;\")\n",
        "union_queries = [f\"SELECT * FROM read_parquet('{f}')\" for f in geo_parquet_files]\n",
        "query_manzanas = \" UNION ALL \".join(union_queries)\n",
        "con.execute(f\"CREATE TABLE manzanas AS {query_manzanas};\")\n",
        "print(\"Tabla 'manzanas' creada con datos geográficos de todos los estados.\")"
      ],
      "metadata": {
        "id": "-cXecoWllMDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------\n",
        "# 7. Unir Datos Censales y Geográficos (censo_geo)\n",
        "# --------------------------------------------------\n",
        "\n",
        "# Se crea un campo CVEGEO a partir de los campos censales y se une con la geometría\n",
        "con.execute(\"DROP TABLE IF EXISTS censo_geo;\")\n",
        "cols = [row[0] for row in con.execute(\"DESCRIBE censo_all;\").fetchall()]\n",
        "exclude = ['ENTIDAD', 'MUN', 'LOC', 'AGEB', 'MZA']\n",
        "select_cols = [col for col in cols if col not in exclude]\n",
        "select_cols_str = ', '.join(['c.' + col for col in select_cols])\n",
        "\n",
        "con.execute(f\"\"\"\n",
        "    CREATE TABLE censo_geo AS\n",
        "    WITH censo AS (\n",
        "        SELECT\n",
        "            *,\n",
        "            CONCAT(\n",
        "                LPAD(CAST(ENTIDAD AS VARCHAR), 2, '0'),\n",
        "                LPAD(CAST(MUN AS VARCHAR), 3, '0'),\n",
        "                LPAD(CAST(LOC AS VARCHAR), 4, '0'),\n",
        "                AGEB,\n",
        "                LPAD(CAST(MZA AS VARCHAR), 3, '0')\n",
        "            ) AS CVEGEO\n",
        "        FROM censo_all\n",
        "    ),\n",
        "    shp AS (\n",
        "        SELECT CVEGEO, geometry FROM manzanas\n",
        "    )\n",
        "    SELECT c.CVEGEO, {select_cols_str}, s.geometry\n",
        "    FROM censo c\n",
        "    LEFT JOIN shp s USING (CVEGEO)\n",
        "    WHERE s.CVEGEO IS NOT NULL;\n",
        "\"\"\")\n",
        "print(\"Tabla 'censo_geo' creada uniendo datos censales y geográficos.\")"
      ],
      "metadata": {
        "id": "VhTxzRYOlM17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------\n",
        "# 8. Conversión de Campos a INTEGER en censo_geo (tabla censo_geo_int)\n",
        "# --------------------------------------------------\n",
        "\n",
        "fields_to_int = [\n",
        "    \"POBFEM\", \"POBMAS\", \"P_0A2\", \"P_0A2_F\", \"P_0A2_M\", \"P_3YMAS\", \"P_3YMAS_F\", \"P_3YMAS_M\",\n",
        "    \"P_5YMAS\", \"P_5YMAS_F\", \"P_5YMAS_M\", \"P_12YMAS\", \"P_12YMAS_F\", \"P_12YMAS_M\", \"P_15YMAS\", \"P_15YMAS_F\",\n",
        "    \"P_15YMAS_M\", \"P_18YMAS\", \"P_18YMAS_F\", \"P_18YMAS_M\", \"P_3A5\", \"P_3A5_F\", \"P_3A5_M\", \"P_6A11\",\n",
        "    \"P_6A11_F\", \"P_6A11_M\", \"P_8A14\", \"P_8A14_F\", \"P_8A14_M\", \"P_12A14\", \"P_12A14_F\", \"P_12A14_M\",\n",
        "    \"P_15A17\", \"P_15A17_F\", \"P_15A17_M\", \"P_18A24\", \"P_18A24_F\", \"P_18A24_M\", \"P_15A49_F\", \"P_60YMAS\",\n",
        "    \"P_60YMAS_F\", \"P_60YMAS_M\", \"REL_H_M\", \"POB0_14\", \"POB15_64\", \"POB65_MAS\", \"PROM_HNV\", \"PNACENT\",\n",
        "    \"PNACENT_F\", \"PNACENT_M\", \"PNACOE\", \"PNACOE_F\", \"PNACOE_M\", \"PRES2015\", \"PRES2015_F\", \"PRES2015_M\",\n",
        "    \"PRESOE15\", \"PRESOE15_F\", \"PRESOE15_M\", \"P3YM_HLI\", \"P3YM_HLI_F\", \"P3YM_HLI_M\", \"P3HLINHE\", \"P3HLINHE_F\",\n",
        "    \"P3HLINHE_M\", \"P3HLI_HE\", \"P3HLI_HE_F\", \"P3HLI_HE_M\", \"P5_HLI\", \"P5_HLI_NHE\", \"P5_HLI_HE\", \"PHOG_IND\",\n",
        "    \"POB_AFRO\", \"POB_AFRO_F\", \"POB_AFRO_M\", \"PCON_DISC\", \"PCDISC_MOT\", \"PCDISC_VIS\", \"PCDISC_LENG\",\n",
        "    \"PCDISC_AUD\", \"PCDISC_MOT2\", \"PCDISC_MEN\", \"PCON_LIMI\", \"PCLIM_CSB\", \"PCLIM_VIS\", \"PCLIM_HACO\",\n",
        "    \"PCLIM_OAUD\", \"PCLIM_MOT2\", \"PCLIM_RE_CO\", \"PCLIM_PMEN\", \"PSIND_LIM\", \"P3A5_NOA\", \"P3A5_NOA_F\",\n",
        "    \"P3A5_NOA_M\", \"P6A11_NOA\", \"P6A11_NOAF\", \"P6A11_NOAM\", \"P12A14NOA\", \"P12A14NOAF\", \"P12A14NOAM\",\n",
        "    \"P15A17A\", \"P15A17A_F\", \"P15A17A_M\", \"P18A24A\", \"P18A24A_F\", \"P18A24A_M\", \"P8A14AN\", \"P8A14AN_F\",\n",
        "    \"P8A14AN_M\", \"P15YM_AN\", \"P15YM_AN_F\", \"P15YM_AN_M\", \"P15YM_SE\", \"P15YM_SE_F\", \"P15YM_SE_M\",\n",
        "    \"P15PRI_IN\", \"P15PRI_INF\", \"P15PRI_INM\", \"P15PRI_CO\", \"P15PRI_COF\", \"P15PRI_COM\", \"P15SEC_IN\",\n",
        "    \"P15SEC_INF\", \"P15SEC_INM\", \"P15SEC_CO\", \"P15SEC_COF\", \"P15SEC_COM\", \"P18YM_PB\", \"P18YM_PB_F\",\n",
        "    \"P18YM_PB_M\", \"GRAPROES\", \"GRAPROES_F\", \"GRAPROES_M\", \"PEA\", \"PEA_F\", \"PEA_M\", \"PE_INAC\",\n",
        "    \"PE_INAC_F\", \"PE_INAC_M\", \"POCUPADA\", \"POCUPADA_F\", \"POCUPADA_M\", \"PDESOCUP\", \"PDESOCUP_F\",\n",
        "    \"PDESOCUP_M\", \"PSINDER\", \"PDER_SS\", \"PDER_IMSS\", \"PDER_ISTE\", \"PDER_ISTEE\", \"PAFIL_PDOM\",\n",
        "    \"PDER_SEGP\", \"PDER_IMSSB\", \"PAFIL_IPRIV\", \"PAFIL_OTRAI\", \"P12YM_SOLT\", \"P12YM_CASA\", \"P12YM_SEPA\",\n",
        "    \"PCATOLICA\", \"PRO_CRIEVA\", \"POTRAS_REL\", \"PSIN_RELIG\", \"TOTHOG\", \"HOGJEF_F\", \"HOGJEF_M\",\n",
        "    \"POBHOG\", \"PHOGJEF_F\", \"PHOGJEF_M\", \"TVIVHAB\", \"TVIVPAR\", \"VIVPAR_HAB\", \"VIVPARH_CV\",\n",
        "    \"TVIVPARHAB\", \"VIVPAR_DES\", \"VIVPAR_UT\", \"OCUPVIVPAR\", \"PROM_OCUP\", \"PRO_OCUP_C\", \"VPH_PISODT\",\n",
        "    \"VPH_PISOTI\", \"VPH_1DOR\", \"VPH_2YMASD\", \"VPH_1CUART\", \"VPH_2CUART\", \"VPH_3YMASC\", \"VPH_C_ELEC\",\n",
        "    \"VPH_S_ELEC\", \"VPH_AGUADV\", \"VPH_AEASP\", \"VPH_AGUAFV\", \"VPH_TINACO\", \"VPH_CISTER\", \"VPH_EXCSA\",\n",
        "    \"VPH_LETR\", \"VPH_DRENAJ\", \"VPH_NODREN\", \"VPH_C_SERV\", \"VPH_NDEAED\", \"VPH_DSADMA\", \"VPH_NDACMM\",\n",
        "    \"VPH_SNBIEN\", \"VPH_REFRI\", \"VPH_LAVAD\", \"VPH_HMICRO\", \"VPH_AUTOM\", \"VPH_MOTO\", \"VPH_BICI\",\n",
        "    \"VPH_RADIO\", \"VPH_TV\", \"VPH_PC\", \"VPH_TELEF\", \"VPH_CEL\", \"VPH_INTER\", \"VPH_STVP\", \"VPH_SPMVPI\",\n",
        "    \"VPH_CVJ\", \"VPH_SINRTV\", \"VPH_SINLTC\", \"VPH_SINCINT\", \"VPH_SINTIC\"\n",
        "]\n",
        "null_values = ['N/A', 'N/D', '*', '']\n",
        "\n",
        "cols = [row[0] for row in con.execute(\"DESCRIBE censo_geo\").fetchall()]\n",
        "select_expr = []\n",
        "for col in cols:\n",
        "    if col in fields_to_int:\n",
        "        expr = f\"\"\"CASE\n",
        "            WHEN {col} IN ('N/A', 'N/D', '*', '') THEN NULL\n",
        "            ELSE CAST({col} AS INTEGER)\n",
        "        END AS {col}\"\"\"\n",
        "        select_expr.append(expr)\n",
        "    else:\n",
        "        select_expr.append(col)\n",
        "select_expr_str = \",\\n\".join(select_expr)\n",
        "\n",
        "con.execute(\"DROP TABLE IF EXISTS censo_geo_int;\")\n",
        "con.execute(f\"\"\"\n",
        "CREATE TABLE censo_geo_int AS\n",
        "SELECT\n",
        "    {select_expr_str}\n",
        "FROM censo_geo;\n",
        "\"\"\")\n",
        "print(\"Tabla 'censo_geo_int' creada con campos numéricos convertidos a INTEGER donde corresponde.\")\n",
        "\n",
        "# (Opcional) Copiar censo_geo_int a una nueva base de datos\n",
        "nuevo_db_file = \"datos_censo_nacional.duckdb\"\n",
        "if os.path.exists(nuevo_db_file):\n",
        "    os.remove(nuevo_db_file)\n",
        "con.execute(f\"ATTACH '{nuevo_db_file}' AS nuevo_db;\")\n",
        "con.execute(\"CREATE TABLE nuevo_db.censo_geo_int AS SELECT * FROM censo_geo_int;\")\n",
        "con.execute(\"DETACH nuevo_db;\")\n",
        "print(f\"La tabla 'censo_geo_int' ha sido copiada a: {nuevo_db_file}\")"
      ],
      "metadata": {
        "id": "d1pTG73OmDRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ImaEtguSp3Ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNubNDkQcHUl"
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------------\n",
        "# 9. Generación de la Grilla Hexagonal\n",
        "# --------------------------------------------------\n",
        "\n",
        "# Desconectar la conexión actual (si está abierta)\n",
        "con.close()\n",
        "\n",
        "# Conectar a la base de datos 'datos_censo_nacional.duckdb'\n",
        "nuevo_db_file = \"datos_censo_nacional.duckdb\"\n",
        "con_nuevo = duckdb.connect(nuevo_db_file)\n",
        "con_nuevo.execute(\"INSTALL spatial;\")\n",
        "con_nuevo.execute(\"LOAD spatial;\")\n",
        "\n",
        "\n",
        "# Extraer centroides de las geometrías de censo_geo_int en formato WKB\n",
        "query = \"\"\"\n",
        "SELECT\n",
        "    ST_AsWKB(ST_Centroid(geometry)) AS geometry\n",
        "FROM censo_geo_int;\n",
        "\"\"\"\n",
        "df = con_nuevo.execute(query).fetchdf()\n",
        "df[\"geometry\"] = df[\"geometry\"].apply(lambda x: wkb.loads(bytes(x)) if x is not None else None)\n",
        "gdf_centroids = gpd.GeoDataFrame(df, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
        "print(\"Centroides extraídos:\", gdf_centroids.head())\n",
        "\n",
        "# Reproyectar a EPSG:6372 y generar grilla hexagonal con circumradio 1000\n",
        "gdf_centroids_6372 = gdf_centroids.to_crs(epsg=6372)\n",
        "hex_grid = ghg.make_grid_from_gdf(gdf_centroids_6372, R=1000)\n",
        "hex_grid_4326 = hex_grid.to_crs(epsg=4326)\n",
        "print(\"Grid hexagonal generado:\", hex_grid_4326.head())\n",
        "\n",
        "# Exportar grid a Shapefile y Parquet\n",
        "hex_grid_4326.to_file(\"grid_nacional.shp\")\n",
        "grid_parquet_file = \"hex_grid_1km_4326.parquet\"\n",
        "hex_grid_4326.to_parquet(grid_parquet_file)\n",
        "\n",
        "\n",
        "# Asumimos que 'grid_parquet_file' contiene el nombre del archivo Parquet con el grid hexagonal\n",
        "grid_parquet_file = \"hex_grid_1km_4326.parquet\"\n",
        "\n",
        "# Crear (o reemplazar) la tabla 'hex_grid_1km_4326' en la nueva base de datos\n",
        "con_nuevo.execute(f\"\"\"\n",
        "CREATE OR REPLACE TABLE hex_grid_1km_4326 AS\n",
        "SELECT * FROM read_parquet('{grid_parquet_file}')\n",
        "\"\"\")\n",
        "\n",
        "# Mostrar información de la tabla y algunos registros para verificar\n",
        "df_info = con_nuevo.execute(\"PRAGMA table_info('hex_grid_1km_4326');\").fetchdf()\n",
        "print(\"Información de la tabla hex_grid_1km_4326:\")\n",
        "print(df_info)\n",
        "\n",
        "df_first10 = con_nuevo.execute(\"SELECT * FROM hex_grid_1km_4326 LIMIT 10;\").fetchdf()\n",
        "print(\"Primeros 10 registros de hex_grid_1km_4326:\")\n",
        "print(df_first10)\n",
        "\n",
        "con_nuevo.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import duckdb\n",
        "import geopandas as gpd\n",
        "import shapely\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Conectar a la base de datos\n",
        "with duckdb.connect(\"/content/datos_censo_nacional.duckdb\") as con:  # Usar 'with' para asegurar cierre\n",
        "    con.execute(\"INSTALL spatial;\")\n",
        "    con.execute(\"LOAD spatial;\")\n",
        "\n",
        "    con.execute(\"DROP TABLE IF EXISTS censo_centroids;\")\n",
        "\n",
        "    # Precalcular centroides y crear tabla/índices\n",
        "    con.execute(\"\"\"\n",
        "        CREATE TABLE censo_centroids AS\n",
        "        SELECT\n",
        "            *,  -- Selecciona las columnas necesarias\n",
        "            ST_Centroid(geometry) AS centroid\n",
        "        FROM censo_geo_int;\n",
        "    \"\"\")\n",
        "\n",
        "    con.execute(\"DROP INDEX IF EXISTS idx_censo_centroid;\")\n",
        "    con.execute(\"DROP INDEX IF EXISTS idx_hex_geom;\")\n",
        "    con.execute(\"CREATE INDEX idx_censo_centroid ON censo_centroids USING RTREE(centroid);\")\n",
        "    con.execute(\"CREATE INDEX idx_hex_geom ON hex_grid_1km_4326 USING RTREE(geometry);\")\n",
        "\n",
        "    # Crear la tabla hex_final directamente en DuckDB (CTAS)\n",
        "    con.execute(\"DROP TABLE IF EXISTS hex_final;\")\n",
        "    con.execute(\"\"\"\n",
        "    CREATE TABLE hex_final AS\n",
        "    SELECT\n",
        "        h.cell_id,\n",
        "        ST_AsWKB(h.geometry) AS geometry,\n",
        "        SUM(c.POBTOT) AS PoblacionTotal,\n",
        "        SUM(c.VPH_INTER) AS ViviendasInternet,\n",
        "        SUM(c.VPH_CVJ) AS ViviendasConsola,\n",
        "        SUM(c.VPH_SPMVPI) AS ViviendasStreaming\n",
        "    FROM hex_grid_1km_4326 h\n",
        "    LEFT JOIN censo_centroids c\n",
        "        ON ST_Intersects(c.centroid, h.geometry)  -- Usar centroide precalculado\n",
        "    GROUP BY h.cell_id, h.geometry;\n",
        "    \"\"\")\n",
        "\n",
        "    # Cargar a GeoDataFrame (si es necesario)\n",
        "    hex_final_df = con.execute(\"SELECT * FROM hex_final\").fetchdf()\n",
        "    hex_final_df[\"geometry\"] = hex_final_df[\"geometry\"].apply(lambda x: wkb.loads(bytes(x)) if x is not None else None)\n",
        "    hex_final = gpd.GeoDataFrame(hex_final_df, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
        "\n",
        "\n",
        "    # Graficar (sin cambios)\n",
        "    variable = 'PoblacionTotal'\n",
        "    cmap = 'turbo'\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(20, 20))\n",
        "    hex_final.plot(column=variable, cmap=cmap, linewidth=0.8, ax=ax, edgecolor='0.8')\n",
        "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=hex_final[variable].min(), vmax=hex_final[variable].max()))\n",
        "    sm._A = []\n",
        "    cbar = fig.colorbar(sm, ax=ax)\n",
        "    plt.show()\n",
        "\n",
        "# La conexión se cierra automáticamente gracias al 'with'"
      ],
      "metadata": {
        "id": "0-jJknNsqv6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "import folium  # Asegúrate de importar folium\n",
        "import json\n",
        "\n",
        "variable = 'PoblacionTotal'  # Usar el nombre correcto de la columna\n",
        "\n",
        "# Añade una columna 'index' para usarla en el key_on de Folium\n",
        "# Usa .copy() para evitar SettingWithCopyWarning\n",
        "hex_final = hex_final.copy()\n",
        "hex_final['index'] = hex_final.index.astype(str)\n",
        "\n",
        "\n",
        "# Convierte el GeoDataFrame a JSON\n",
        "gdf_json = hex_final.to_json()\n",
        "\n",
        "# Calcula el centro del mapa usando el centroide de los polígonos\n",
        "centroid = hex_final.geometry.centroid\n",
        "avg_lat = centroid.y.mean()\n",
        "avg_lon = centroid.x.mean()\n",
        "\n",
        "# Crea el objeto de mapa de Folium\n",
        "m = folium.Map(location=[avg_lat, avg_lon], zoom_start=13)\n",
        "\n",
        "# Agrega la capa Choropleth\n",
        "folium.Choropleth(\n",
        "    geo_data=gdf_json,\n",
        "    name='choropleth',\n",
        "    data=hex_final,\n",
        "    columns=['index', variable],  # Usar 'index' y el nombre correcto de la variable\n",
        "    key_on='feature.id',  # Usar feature.id, ya que to_json() lo asigna automáticamente\n",
        "    fill_color='YlGnBu',\n",
        "    fill_opacity=0.55,\n",
        "    line_opacity=0.1,\n",
        "    legend_name=variable\n",
        ").add_to(m)\n",
        "\n",
        "# Agrega control de capas (opcional)\n",
        "folium.LayerControl().add_to(m)\n",
        "\n",
        "# Muestra el mapa\n",
        "m"
      ],
      "metadata": {
        "id": "boVm7LOICMSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0YOK0AtXCSi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uHCKRyJ3upPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gz-5ltLXNgl6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}