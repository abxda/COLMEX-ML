{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPxuQlO8aEpBD9ilN/TF1gb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abxda/COLMEX-ML/blob/main/Semana_12_RAG_COLMEX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiRGLUcu91rw"
      },
      "outputs": [],
      "source": [
        "!echo \"--- Instalando paquetes necesarios ---\"\n",
        "!pip install -U duckduckgo_search wordcloud matplotlib pandas nltk spacy sentence-transformers flagembedding ollama chromadb seaborn scikit-learn --quiet\n",
        "!echo \"--- Descargando modelo spaCy en español ---\"\n",
        "!python -m spacy download es_core_news_sm --quiet\n",
        "!echo \"--- Instalaciones completadas ---\"\n",
        "!echo \"NOTA: REINICIAR EL ENTORNO DE EJECUCIÓN.\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 2. IMPORTACIONES\n",
        "# ==============================================================================\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "import string\n",
        "import json\n",
        "\n",
        "# Procesamiento de datos y NLP\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "\n",
        "# Búsqueda y Visualización\n",
        "from duckduckgo_search import DDGS\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine Learning: Vectorización, Clustering, Reducción de Dimensionalidad\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import DBSCAN, KMeans\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Modelos de Embeddings\n",
        "from FlagEmbedding import BGEM3FlagModel\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# LLM y Base de Datos Vectorial\n",
        "import ollama\n",
        "import chromadb\n"
      ],
      "metadata": {
        "id": "GVgOFIdR-aEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 3. CONFIGURACIÓN INICIAL Y DESCARGAS ADICIONALES\n",
        "# ==============================================================================\n",
        "\n",
        "# Descargar recursos de NLTK (stopwords y tokenizador punkt)\n",
        "print(\"--- Descargando recursos de NLTK (stopwords, punkt) ---\")\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "print(\"--- Recursos NLTK listos ---\")\n",
        "# Cargar modelo spaCy\n",
        "print(\"--- Cargando modelo spaCy es_core_news_sm ---\")\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "print(\"--- Modelo spaCy cargado ---\")\n",
        "\n",
        "\n",
        "# Modelos de Embedding (se cargarán más adelante justo antes de usarlos)\n",
        "BGE_MODEL_NAME = 'BAAI/bge-m3'\n",
        "E5_MODEL_NAME = 'intfloat/multilingual-e5-large'\n",
        "\n",
        "# Modelos LLM (Ollama)\n",
        "OLLAMA_MODEL_DEEPSEEK = 'deepseek-r1:8b' # Asegúrate de que este modelo esté disponible en tu instancia de Ollama\n",
        "OLLAMA_MODEL_GEMMA = 'gemma3:12b'      # Asegúrate de que este modelo esté disponible\n",
        "DEFAULT_OLLAMA_MODEL = OLLAMA_MODEL_GEMMA # Modelo a usar por defecto\n",
        "\n",
        "# Nombre para la colección ChromaDB\n",
        "CHROMA_COLLECTION_NAME = \"news_rag_e5_clean\"\n",
        "\n",
        "# Político para análisis estructurado\n",
        "POLITICO_ANALISIS = \"Claudia Sheinbaum\""
      ],
      "metadata": {
        "id": "j2QlMfiS-L2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 4. FUNCIONES AUXILIARES\n",
        "# ==============================================================================\n",
        "\n",
        "def buscar_noticias(keywords, region, max_results=100):\n",
        "    \"\"\"Busca noticias usando la versión síncrona de DDGS.\"\"\"\n",
        "    print(f\"Buscando {max_results} noticias para '{keywords}' en región '{region}'...\")\n",
        "    with DDGS() as ddgs:\n",
        "        noticias = list(ddgs.news(\n",
        "            keywords=keywords,\n",
        "            region=region,\n",
        "            safesearch='moderate',\n",
        "            max_results=max_results\n",
        "        ))\n",
        "    print(f\"Se encontraron {len(noticias)} noticias.\")\n",
        "    return noticias\n",
        "\n",
        "def limpiar_texto(text):\n",
        "    \"\"\"Convierte a minúsculas, quita puntuación y stopwords en español.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    palabras = text.split()\n",
        "    stop_words_es = set(stopwords.words('spanish'))\n",
        "    palabras_limpias = [word for word in palabras if word not in stop_words_es]\n",
        "    return ' '.join(palabras_limpias)\n",
        "\n",
        "def lematizar_texto(text, nlp_model):\n",
        "    \"\"\"Lematiza el texto usando un modelo spaCy.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    doc = nlp_model(text)\n",
        "    return ' '.join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n",
        "\n",
        "def generar_nube_de_palabras(textos, ax, titulo=\"Nube de Palabras\"):\n",
        "    \"\"\"Genera y muestra una nube de palabras en un eje Matplotlib.\"\"\"\n",
        "    texto_completo = ' '.join(textos)\n",
        "    if not texto_completo.strip():\n",
        "        print(f\"Advertencia: No hay texto para generar la nube de palabras '{titulo}'.\")\n",
        "        ax.text(0.5, 0.5, 'No hay datos suficientes', horizontalalignment='center', verticalalignment='center')\n",
        "        ax.axis('off')\n",
        "        return\n",
        "\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(texto_completo)\n",
        "    ax.imshow(wordcloud, interpolation='bilinear')\n",
        "    ax.set_title(titulo)\n",
        "    ax.axis('off')\n",
        "\n",
        "def visualizar_clusters(df, cluster_labels, textos_col, titulo_col, fuente_col, num_clusters_a_mostrar=None):\n",
        "    \"\"\"Visualiza clusters con nubes de palabras y títulos/fuentes.\"\"\"\n",
        "    df['cluster'] = cluster_labels\n",
        "    cluster_counts = df['cluster'].value_counts()\n",
        "    print(\"Conteo de noticias por cluster:\")\n",
        "    print(cluster_counts)\n",
        "\n",
        "    # Filtrar clusters de ruido (-1 en DBSCAN) y limitar número si se especifica\n",
        "    clusters_validos = cluster_counts[cluster_counts.index != -1].index\n",
        "    if num_clusters_a_mostrar is not None:\n",
        "        clusters_validos = clusters_validos[:num_clusters_a_mostrar]\n",
        "\n",
        "    num_clusters = len(clusters_validos)\n",
        "    if num_clusters == 0:\n",
        "        print(\"No se encontraron clusters válidos para visualizar.\")\n",
        "        return\n",
        "\n",
        "    fig, axs = plt.subplots(num_clusters, 2, figsize=(20, 6 * num_clusters))\n",
        "    # Asegurarse que axs es siempre un array 2D\n",
        "    if num_clusters == 1:\n",
        "        axs = np.array([axs])\n",
        "\n",
        "    for i, cluster_id in enumerate(clusters_validos):\n",
        "        cluster_data = df[df['cluster'] == cluster_id]\n",
        "        textos_cluster = cluster_data[textos_col].tolist()\n",
        "        titulos_fuentes_cluster = cluster_data[[titulo_col, fuente_col]].apply(\n",
        "            lambda x: f\"- {x[titulo_col]} ({x[fuente_col]})\", axis=1\n",
        "        ).tolist()\n",
        "\n",
        "        # Generar nube de palabras\n",
        "        generar_nube_de_palabras(textos_cluster, axs[i, 0], titulo=f'Cluster {cluster_id} - Nube de Palabras')\n",
        "\n",
        "        # Mostrar títulos y fuentes\n",
        "        axs[i, 1].text(0.05, 0.95, \"\\n\".join(titulos_fuentes_cluster),\n",
        "                       verticalalignment='top', horizontalalignment='left',\n",
        "                       fontsize=9, family='monospace', wrap=True)\n",
        "        axs[i, 1].set_title(f'Cluster {cluster_id} - Títulos y Fuentes ({len(titulos_fuentes_cluster)} noticias)')\n",
        "        axs[i, 1].axis('off')\n",
        "\n",
        "    plt.tight_layout(pad=3.0)\n",
        "    plt.show()\n",
        "\n",
        "def visualizar_tsne(X_embedded, df_metadata, titulo_col, plot_title):\n",
        "    \"\"\"Realiza t-SNE y visualiza los embeddings con etiquetas.\"\"\"\n",
        "    print(f\"--- Realizando t-SNE para {plot_title} ---\")\n",
        "    if X_embedded.shape[0] <= 1:\n",
        "        print(\"Se necesita más de 1 punto de datos para t-SNE.\")\n",
        "        return\n",
        "    # Ajustar perplexity si hay pocos datos\n",
        "    perplexity_value = min(30, X_embedded.shape[0] - 1)\n",
        "\n",
        "    tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity_value, n_iter=300) # Reducir iteraciones si es lento\n",
        "    X_2d = tsne.fit_transform(X_embedded)\n",
        "\n",
        "    df_tsne = pd.DataFrame(X_2d, columns=['x', 'y'], index=df_metadata.index)\n",
        "    df_tsne['label'] = df_metadata[titulo_col] # Asegúrate que titulo_col es el nombre correcto ('title')\n",
        "\n",
        "    plt.figure(figsize=(18, 12))\n",
        "    scatter = sns.scatterplot(data=df_tsne, x='x', y='y', hue='label', palette='viridis', legend=False, alpha=0.7, s=50)\n",
        "\n",
        "    # --- INICIO DE LA SECCIÓN CORREGIDA ---\n",
        "    # Añadir etiquetas (puede ser lento/denso con muchas noticias)\n",
        "    # Considera mostrar solo una fracción o en hover interactivo en otros entornos\n",
        "    for i in df_tsne.index:\n",
        "        plt.text(df_tsne.loc[i, 'x'] + 0.05, df_tsne.loc[i, 'y'] + 0.05, # Pequeño offset\n",
        "                 df_tsne.loc[i, 'label'][:50] + \"...\", # Acortar etiqueta a 50 caracteres\n",
        "                 fontsize=8, alpha=0.7)\n",
        "    # --- FIN DE LA SECCIÓN CORREGIDA ---\n",
        "\n",
        "    plt.title(plot_title)\n",
        "    plt.xlabel('Componente t-SNE 1')\n",
        "    plt.ylabel('Componente t-SNE 2')\n",
        "    plt.grid(True, linestyle='--', alpha=0.5)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "62tPQDbK-kY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuraciones para la búsqueda de noticias\n",
        "configuraciones = [\n",
        "    {\"idioma\": \"Español\", \"keywords\": \"México\", \"region\": \"mx-es\"}\n",
        "]"
      ],
      "metadata": {
        "id": "bXeIiZ6Mc_7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 5. BÚSQUEDA Y PROCESAMIENTO INICIAL DE NOTICIAS\n",
        "# ==============================================================================\n",
        "\n",
        "# Realizar la búsqueda\n",
        "todas_las_noticias = []\n",
        "for conf in configuraciones:\n",
        "    noticias = buscar_noticias(conf['keywords'], conf['region'])\n",
        "    todas_las_noticias.extend(noticias)\n",
        "    print(f\"Resultados acumulados: {len(todas_las_noticias)} noticias.\")\n",
        "\n",
        "# Crear DataFrame\n",
        "df_noticias = pd.DataFrame(todas_las_noticias)\n",
        "\n",
        "if not df_noticias.empty and 'body' in df_noticias.columns and 'title' in df_noticias.columns:\n",
        "    print(\"\\n--- DataFrame inicial de noticias ---\")\n",
        "    print(df_noticias.head())\n",
        "    print(f\"\\nDimensiones del DataFrame: {df_noticias.shape}\")\n",
        "\n",
        "    # Limpieza básica y lematización\n",
        "    print(\"\\n--- Limpiando y Lematizando texto ---\")\n",
        "    df_noticias['clean_body'] = df_noticias['body'].apply(limpiar_texto)\n",
        "    df_noticias['lemmatized_body'] = df_noticias['clean_body'].apply(lambda x: lematizar_texto(x, nlp))\n",
        "\n",
        "    print(\"\\n--- DataFrame con texto limpio y lematizado ---\")\n",
        "    print(df_noticias[['title', 'clean_body', 'lemmatized_body']].head())\n",
        "\n",
        "    # Generar Nube de Palabras Inicial (sobre texto lematizado)\n",
        "    print(\"\\n--- Generando Nube de Palabras (Lematizada) ---\")\n",
        "    fig_wc, ax_wc = plt.subplots(1, 1, figsize=(10, 5))\n",
        "    generar_nube_de_palabras(df_noticias['lemmatized_body'].dropna(), ax_wc, \"Nube de Palabras Global (Lematizada)\")\n",
        "    plt.show()\n",
        "\n",
        "    # Análisis de Fuentes\n",
        "    print(\"\\n--- Analizando distribución de fuentes ---\")\n",
        "    if 'source' in df_noticias.columns:\n",
        "        conteo_fuentes = df_noticias['source'].value_counts()\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        top_n = 20 # Mostrar solo las top N fuentes\n",
        "        conteo_fuentes.head(top_n).plot(kind='bar', color='skyblue')\n",
        "        plt.title(f'Top {top_n} Fuentes de Noticias')\n",
        "        plt.xlabel('Fuente')\n",
        "        plt.ylabel('Cantidad de Noticias')\n",
        "        plt.xticks(rotation=75, ha='right')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Advertencia: Columna 'source' no encontrada para análisis de fuentes.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n--- No se encontraron noticias o el DataFrame no tiene las columnas 'body'/'title'. Saltando análisis detallado. ---\")\n",
        "    # Salir o manejar el caso de no noticias si es necesario\n",
        "    exit() # O gestionarlo de otra forma"
      ],
      "metadata": {
        "id": "AEM9oj4C-pvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 6. CLUSTERING CON TF-IDF\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" 6. CLUSTERING CON TF-IDF (N-gramas de caracteres)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "vectorizer_tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3, 5))\n",
        "X_tfidf = vectorizer_tfidf.fit_transform(df_noticias['lemmatized_body'].dropna())\n",
        "X_tfidf_normalized = normalize(X_tfidf)\n",
        "\n",
        "print(f\"Dimensiones de la matriz TF-IDF: {X_tfidf_normalized.shape}\")\n",
        "\n",
        "# --- Clustering con K-Means (TF-IDF) ---\n",
        "print(\"\\n--- Clustering con K-Means (TF-IDF) ---\")\n",
        "kmeans_tfidf = KMeans(n_clusters=8, random_state=42, n_init=10)\n",
        "clusters_kmeans_tfidf = kmeans_tfidf.fit_predict(X_tfidf_normalized)\n",
        "visualizar_clusters(df_noticias, clusters_kmeans_tfidf, 'lemmatized_body', 'title', 'source', num_clusters_a_mostrar=8)\n",
        "\n",
        "# --- Clustering con DBSCAN (TF-IDF) ---\n",
        "print(\"\\n--- Clustering con DBSCAN (TF-IDF) ---\")\n",
        "# Nota: eps puede necesitar ajuste según los datos\n",
        "dbscan_tfidf = DBSCAN(eps=0.8, min_samples=2, metric='cosine')\n",
        "clusters_dbscan_tfidf = dbscan_tfidf.fit_predict(X_tfidf_normalized)\n",
        "visualizar_clusters(df_noticias, clusters_dbscan_tfidf, 'lemmatized_body', 'title', 'source')"
      ],
      "metadata": {
        "id": "Cf0_Ct1L--N5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 7. CLUSTERING CON EMBEDDINGS BGE-M3\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\" 7. CLUSTERING CON EMBEDDINGS BGE-M3 ({BGE_MODEL_NAME})\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"--- Cargando modelo BGE: {BGE_MODEL_NAME} ---\")\n",
        "model_bge = BGEM3FlagModel(BGE_MODEL_NAME) # Carga puede tardar\n",
        "print(\"--- Modelo BGE cargado ---\")\n",
        "\n",
        "print(\"--- Generando embeddings BGE-M3 para 'clean_body' ---\")\n",
        "embeddings_bge_list = []\n",
        "texts_to_embed_bge = df_noticias['clean_body'].dropna().tolist()\n",
        "if texts_to_embed_bge:\n",
        "     # BGE espera una lista, encode devuelve dict con 'dense_vecs'\n",
        "    embeddings_bge = model_bge.encode(texts_to_embed_bge, batch_size=12, max_length=512)['dense_vecs'] # Ajustar batch_size según memoria\n",
        "    X_bge = np.array(embeddings_bge)\n",
        "    print(f\"Dimensiones de los embeddings BGE: {X_bge.shape}\")\n",
        "\n",
        "    # --- Clustering con K-Means (BGE) ---\n",
        "    print(\"\\n--- Clustering con K-Means (BGE) ---\")\n",
        "    kmeans_bge = KMeans(n_clusters=8, random_state=42, n_init=10)\n",
        "    clusters_kmeans_bge = kmeans_bge.fit_predict(X_bge) # BGE ya suele estar normalizado\n",
        "    visualizar_clusters(df_noticias.dropna(subset=['clean_body']), clusters_kmeans_bge, 'clean_body', 'title', 'source', num_clusters_a_mostrar=8)\n",
        "\n",
        "    # --- Clustering con DBSCAN (BGE) ---\n",
        "    print(\"\\n--- Clustering con DBSCAN (BGE) ---\")\n",
        "\n",
        "    dbscan_bge = DBSCAN(eps=0.365, min_samples=2, metric='cosine') # Usar cosine suele ser mejor para high-dim embeddings\n",
        "    clusters_dbscan_bge = dbscan_bge.fit_predict(X_bge)\n",
        "    visualizar_clusters(df_noticias.dropna(subset=['clean_body']), clusters_dbscan_bge, 'clean_body', 'title', 'source')\n",
        "\n",
        "    # --- Visualización t-SNE (BGE) ---\n",
        "    visualizar_tsne(X_bge, df_noticias.dropna(subset=['clean_body']), 'title', 't-SNE de Embeddings BGE-M3')\n",
        "\n",
        "else:\n",
        "    print(\"No hay textos limpios para generar embeddings BGE.\")\n"
      ],
      "metadata": {
        "id": "vl-Frc47--Qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 8. CLUSTERING CON EMBEDDINGS MULTILINGUAL-E5\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\" 8. CLUSTERING CON EMBEDDINGS E5 ({E5_MODEL_NAME})\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"--- Cargando modelo E5: {E5_MODEL_NAME} ---\")\n",
        "model_e5 = SentenceTransformer(E5_MODEL_NAME)\n",
        "print(\"--- Modelo E5 cargado ---\")\n",
        "\n",
        "print(\"--- Generando embeddings E5 para 'clean_body' (con prefijo 'passage:') ---\")\n",
        "texts_to_embed_e5_raw = df_noticias['clean_body'].dropna().tolist()\n",
        "if texts_to_embed_e5_raw:\n",
        "    texts_to_embed_e5 = [\"passage: \" + text for text in texts_to_embed_e5_raw]\n",
        "    embeddings_e5 = model_e5.encode(texts_to_embed_e5, show_progress_bar=True, batch_size=32) # Ajustar batch_size\n",
        "    X_e5 = np.array(embeddings_e5)\n",
        "    X_e5_normalized = normalize(X_e5, norm='l2', axis=1) # E5 recomienda normalizar\n",
        "    print(f\"Dimensiones de los embeddings E5 normalizados: {X_e5_normalized.shape}\")\n",
        "\n",
        "    # --- Clustering con K-Means (E5) ---\n",
        "    print(\"\\n--- Clustering con K-Means (E5) ---\")\n",
        "    kmeans_e5 = KMeans(n_clusters=8, random_state=42, n_init=10)\n",
        "    clusters_kmeans_e5 = kmeans_e5.fit_predict(X_e5_normalized)\n",
        "    visualizar_clusters(df_noticias.dropna(subset=['clean_body']), clusters_kmeans_e5, 'clean_body', 'title', 'source', num_clusters_a_mostrar=8)\n",
        "\n",
        "    # --- Clustering con DBSCAN (E5) ---\n",
        "    print(\"\\n--- Clustering con DBSCAN (E5) ---\")\n",
        "    # Eps ajustado basado en el ejemplo original, puede necesitar tuning\n",
        "    dbscan_e5 = DBSCAN(eps=0.365, min_samples=2, metric='cosine') # Usar cosine con normalizados\n",
        "    clusters_dbscan_e5 = dbscan_e5.fit_predict(X_e5_normalized)\n",
        "    visualizar_clusters(df_noticias.dropna(subset=['clean_body']), clusters_dbscan_e5, 'clean_body', 'title', 'source')\n",
        "\n",
        "    # --- Visualización t-SNE (E5) ---\n",
        "    visualizar_tsne(X_e5_normalized, df_noticias.dropna(subset=['clean_body']), 'title', f't-SNE de Embeddings {E5_MODEL_NAME}')\n",
        "\n",
        "else:\n",
        "    print(\"No hay textos limpios para generar embeddings E5.\")\n"
      ],
      "metadata": {
        "id": "cWmOOARC--Sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "!pip install -q ollama chromadb"
      ],
      "metadata": {
        "id": "9Oi0kfM9BdzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Configurando y iniciando Ollama Server ---\")\n",
        "# Lanzar el servidor Ollama en segundo plano\n",
        "# Usamos nohup y & para que siga corriendo aunque cerremos la terminal (o se desconecte Colab)\n",
        "ollama_process = subprocess.Popen(['nohup', 'ollama', 'serve'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "# Darle tiempo al servidor para que inicie\n",
        "print(\"Esperando a que el servidor Ollama inicie (10 segundos)...\")\n",
        "time.sleep(10)\n"
      ],
      "metadata": {
        "id": "KE6bhKx9BXwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OLLAMA_MODEL_GEMMA"
      ],
      "metadata": {
        "id": "Epd0UjtjC6wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# 9. CONFIGURACIÓN DE OLLAMA Y CHROMADB PARA RAG\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" 9. CONFIGURACIÓN DE OLLAMA Y CHROMADB PARA RAG\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"--- Verificando conexión con Ollama ---\")\n",
        "try:\n",
        "    ollama.list()\n",
        "    print(\"Ollama server detectado.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: No se pudo conectar con Ollama. Asegúrate de que 'ollama serve' esté corriendo. Detalle: {e}\")\n",
        "    # Podrías decidir salir aquí si Ollama es esencial\n",
        "    # exit()\n",
        "\n",
        "# --- Descargar Modelos Ollama (si no existen) ---\n",
        "def pull_ollama_model(model_name):\n",
        "    print(f\"Verificando/Descargando modelo Ollama: {model_name}...\")\n",
        "    try:\n",
        "        # Verificar si el modelo existe localmente\n",
        "        model_list = ollama.list()['models']\n",
        "        if any(m['model'] == model_name for m in model_list):\n",
        "            print(f\"Modelo '{model_name}' ya existe localmente.\")\n",
        "            return True\n",
        "\n",
        "        # Si no existe, intentar descargarlo\n",
        "        print(f\"Descargando modelo '{model_name}' (puede tardar)...\")\n",
        "        # El pull a través de la librería a veces es menos informativo que el CLI\n",
        "        current_digest = \"\"\n",
        "        for progress in ollama.pull(model_name, stream=True):\n",
        "            digest = progress.get(\"digest\", \"\")\n",
        "            if digest != current_digest and digest != \"\":\n",
        "                print(f\"Pulling {model_name}: {digest} - {progress.get('completed', 0)}/{progress.get('total', 0)}\")\n",
        "                current_digest = digest\n",
        "            elif 'status' in progress and progress['status'] == 'success':\n",
        "                 print(f\"Descarga de '{model_name}' completada.\")\n",
        "                 return True\n",
        "            elif 'error' in progress:\n",
        "                 print(f\"ERROR descargando '{model_name}': {progress['error']}\")\n",
        "                 return False\n",
        "        # Si el stream termina sin 'success' o 'error' explícito (raro)\n",
        "        print(f\"Descarga de '{model_name}' finalizada (estado final desconocido desde stream). Verifique manualmente.\")\n",
        "        return True # Asumir éxito si no hubo error explícito\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR durante la descarga/verificación del modelo '{model_name}': {e}\")\n",
        "        return False\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CU3Ax554--VD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargar los modelos necesarios\n",
        "model_download_ok = pull_ollama_model(OLLAMA_MODEL_DEEPSEEK)"
      ],
      "metadata": {
        "id": "rD64Fde-DDqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_download_ok = pull_ollama_model(OLLAMA_MODEL_GEMMA) and model_download_ok"
      ],
      "metadata": {
        "id": "sbD7E0LNDGiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ollama.list()['models']"
      ],
      "metadata": {
        "id": "WdYBY4sHDiQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configurar ChromaDB e Indexar Datos ---\n",
        "print(\"\\n--- Configurando ChromaDB e Indexando Noticias ---\")\n",
        "data_available_for_rag = False\n",
        "collection = None\n",
        "embedding_model_for_rag = model_e5 # Usaremos E5 para RAG como en el original\n",
        "\n",
        "texts_for_rag = df_noticias['clean_body'].dropna().tolist()\n",
        "ids_for_rag = [f\"news_{i}\" for i, _ in enumerate(texts_for_rag)]\n",
        "\n",
        "if texts_for_rag:\n",
        "    print(f\"Se prepararon {len(texts_for_rag)} textos de noticias para indexar.\")\n",
        "\n",
        "    print(\"Generando embeddings para RAG (usando E5 con prefijo 'passage:')...\")\n",
        "    texts_with_prefix_rag = [\"passage: \" + text for text in texts_for_rag]\n",
        "    try:\n",
        "        news_embeddings_rag = embedding_model_for_rag.encode(\n",
        "            texts_with_prefix_rag,\n",
        "            show_progress_bar=True,\n",
        "            batch_size=32\n",
        "        )\n",
        "\n",
        "        print(\"Inicializando ChromaDB (cliente en memoria)...\")\n",
        "        chroma_client = chromadb.Client() # O PersistentClient(path=\"ruta/a/db\") para persistencia\n",
        "\n",
        "        # Crear o cargar colección (borrándola si existe para limpieza en este script)\n",
        "        print(f\"Intentando crear/recrear colección ChromaDB: '{CHROMA_COLLECTION_NAME}'\")\n",
        "        try:\n",
        "            chroma_client.delete_collection(name=CHROMA_COLLECTION_NAME)\n",
        "            print(f\"Colección existente '{CHROMA_COLLECTION_NAME}' eliminada.\")\n",
        "        except Exception:\n",
        "            print(f\"Colección '{CHROMA_COLLECTION_NAME}' no existía previamente o no se pudo borrar.\")\n",
        "\n",
        "        collection = chroma_client.create_collection(\n",
        "            name=CHROMA_COLLECTION_NAME,\n",
        "            metadata={\"hnsw:space\": \"cosine\"}\n",
        "            )\n",
        "\n",
        "        print(\"Añadiendo documentos a ChromaDB...\")\n",
        "        collection.add(\n",
        "            embeddings=news_embeddings_rag.tolist(), # Chroma espera listas\n",
        "            documents=texts_for_rag, # Guardamos el texto limpio SIN prefijo\n",
        "            ids=ids_for_rag\n",
        "        )\n",
        "        print(f\"--- Datos indexados ({collection.count()} documentos) en ChromaDB. Colección '{CHROMA_COLLECTION_NAME}' lista. ---\")\n",
        "        data_available_for_rag = True\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR durante la generación de embeddings o indexación en ChromaDB: {e}\")\n",
        "        data_available_for_rag = False\n",
        "\n",
        "else:\n",
        "    print(\"--- ADVERTENCIA: No hay textos limpios ('clean_body') disponibles en df_noticias. La función RAG no tendrá contexto. ---\")\n",
        "    data_available_for_rag = False"
      ],
      "metadata": {
        "id": "MiXnW9ONDAxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t43A1i52CuZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 10. FUNCIÓN RAG Y EJEMPLOS\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" 10. FUNCIÓN RAG (Retrieval-Augmented Generation)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def ask_llm_with_rag(query: str, ollama_model_name: str, embed_model, db_collection, k_results: int = 3):\n",
        "    \"\"\"Realiza una consulta RAG: recupera contexto de ChromaDB y pregunta a Ollama.\"\"\"\n",
        "    print(f\"\\n--- Procesando consulta RAG para: '{query}' ---\")\n",
        "    print(f\"--- Usando LLM: {ollama_model_name} ---\")\n",
        "\n",
        "    if db_collection is None or not data_available_for_rag:\n",
        "        print(\"--- ADVERTENCIA: Base de datos no disponible o vacía. Preguntando directamente al modelo sin contexto. ---\")\n",
        "        context_text = \"No hay contexto disponible.\"\n",
        "    else:\n",
        "        # 1. Embeddear la consulta (con prefijo \"query:\")\n",
        "        print(\"Generando embedding para la consulta...\")\n",
        "        try:\n",
        "            query_embedding = embed_model.encode([\"query: \" + query])[0]\n",
        "\n",
        "            # 2. Buscar en ChromaDB\n",
        "            print(f\"Buscando {k_results} documentos relevantes en ChromaDB...\")\n",
        "            results = db_collection.query(\n",
        "                query_embeddings=[query_embedding.tolist()],\n",
        "                n_results=k_results,\n",
        "                include=['documents', 'distances'] # Pedir distancias puede ser útil para debug\n",
        "            )\n",
        "\n",
        "            # 3. Extraer contexto y mostrar info\n",
        "            retrieved_docs = results['documents'][0]\n",
        "            distances = results['distances'][0]\n",
        "            print(\"--- Contexto Recuperado ---\")\n",
        "            context_text = \"\"\n",
        "            for i, doc in enumerate(retrieved_docs):\n",
        "                dist_str = f\"{distances[i]:.4f}\"\n",
        "                print(f\"  Doc {i+1} (Distancia: {dist_str}): {doc[:150]}...\") # Mostrar inicio del doc\n",
        "                context_text += f\"Fragmento {i+1}:\\n{doc}\\n\\n\"\n",
        "            context_text = context_text.strip()\n",
        "            if not context_text:\n",
        "                print(\"Advertencia: No se recuperó ningún documento relevante.\")\n",
        "                context_text = \"No se encontró contexto relevante en la base de datos.\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR durante la recuperación de contexto: {e}\")\n",
        "            context_text = \"Error al recuperar contexto.\"\n",
        "\n",
        "    # 4. Construir el prompt para Ollama\n",
        "    prompt = f\"\"\"Eres un asistente que responde preguntas basándose ESTRICTAMENTE en el contexto proporcionado.\n",
        "Si la información necesaria para responder la pregunta no se encuentra en el contexto, debes indicar CLARAMENTE que no puedes responder con la información dada. No inventes información ni utilices conocimiento externo.\n",
        "\n",
        "Contexto Recuperado de Noticias:\n",
        "--- START OF CONTEXT ---\n",
        "{context_text}\n",
        "--- END OF CONTEXT ---\n",
        "\n",
        "Pregunta del Usuario: {query}\n",
        "\n",
        "Respuesta Basada Únicamente en el Contexto:\"\"\"\n",
        "\n",
        "    print(f\"--- Enviando pregunta y contexto a {ollama_model_name} vía Ollama... ---\")\n",
        "\n",
        "    # 5. Llamar a Ollama\n",
        "    try:\n",
        "        response = ollama.chat(\n",
        "            model=ollama_model_name,\n",
        "            messages=[{'role': 'user', 'content': prompt}],\n",
        "            options={'temperature': 0.1} # Baja temperatura para respuestas basadas en hechos\n",
        "        )\n",
        "        final_answer = response['message']['content']\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR al llamar a Ollama ({ollama_model_name}): {e}\")\n",
        "        final_answer = f\"Error al obtener respuesta del modelo {ollama_model_name}.\"\n",
        "\n",
        "    # 6. Devolver la respuesta del modelo\n",
        "    return final_answer\n",
        "\n",
        "# --- Ejemplos de Uso RAG ---\n",
        "if data_available_for_rag:\n",
        "    user_question = \"¿Qué acciones o declaraciones se mencionan sobre Claudia Sheinbaum en relación con los aranceles o tensiones comerciales entre México y EE.UU.?\"\n",
        "\n",
        "    # Ejemplo con Gemma 3\n",
        "    print(\"\\n--- Ejecutando RAG con Gemma 3 ---\")\n",
        "    final_answer_gemma = ask_llm_with_rag(\n",
        "        query=user_question,\n",
        "        ollama_model_name=OLLAMA_MODEL_GEMMA,\n",
        "        embed_model=embedding_model_for_rag,\n",
        "        db_collection=collection,\n",
        "        k_results=5\n",
        "    )\n",
        "    print(\"\\n--- Respuesta Final de Gemma 3 (RAG) ---\")\n",
        "    print(final_answer_gemma)\n",
        "\n",
        "    # Ejemplo con DeepSeek (si se descargó)\n",
        "    if any(m['model'] == OLLAMA_MODEL_DEEPSEEK for m in ollama.list()['models']):\n",
        "        print(\"\\n--- Ejecutando RAG con DeepSeek ---\")\n",
        "        final_answer_deepseek = ask_llm_with_rag(\n",
        "            query=user_question,\n",
        "            ollama_model_name=OLLAMA_MODEL_DEEPSEEK,\n",
        "            embed_model=embedding_model_for_rag,\n",
        "            db_collection=collection,\n",
        "            k_results=5\n",
        "        )\n",
        "        print(\"\\n--- Respuesta Final de DeepSeek (RAG) ---\")\n",
        "        print(final_answer_deepseek)\n",
        "    else:\n",
        "        print(f\"\\n--- Modelo {OLLAMA_MODEL_DEEPSEEK} no encontrado, saltando ejemplo RAG con DeepSeek. ---\")\n",
        "\n",
        "else:\n",
        "    # Si no hay datos, hacer una pregunta directa como demostración\n",
        "    print(\"\\n--- No hay datos indexados para RAG. Haciendo una pregunta directa al LLM ---\")\n",
        "    user_question_direct = \"¿Qué son los aranceles comerciales?\"\n",
        "    try:\n",
        "        response_direct = ollama.chat(\n",
        "            model=DEFAULT_OLLAMA_MODEL, # Usar el modelo por defecto\n",
        "            messages=[{'role': 'user', 'content': user_question_direct}]\n",
        "        )\n",
        "        print(f\"\\n--- Respuesta Final de {DEFAULT_OLLAMA_MODEL} (Directa) ---\")\n",
        "        print(response_direct['message']['content'])\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR al hacer pregunta directa a {DEFAULT_OLLAMA_MODEL}: {e}\")"
      ],
      "metadata": {
        "id": "_hGYmyWJ_I1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 11. EXTRACCIÓN ESTRUCTURADA DE INFORMACIÓN\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\" 11. EXTRACCIÓN ESTRUCTURADA DE INFORMACIÓN (Enfoque: {POLITICO_ANALISIS})\")\n",
        "print(\"=\"*60)\n",
        "DEFAULT_OLLAMA_MODEL = OLLAMA_MODEL_DEEPSEEK\n",
        "structured_results = []\n",
        "\n",
        "# Verificar si df_noticias existe y tiene las columnas necesarias\n",
        "if 'df_noticias' in locals() and not df_noticias.empty and 'title' in df_noticias.columns and 'body' in df_noticias.columns:\n",
        "    print(f\"Procesando noticias para análisis estructurado sobre: {POLITICO_ANALISIS}\")\n",
        "    print(f\"Usando LLM: {DEFAULT_OLLAMA_MODEL}\")\n",
        "\n",
        "    # Limitar a N noticias para el ejemplo, quitar .head() para procesar todo\n",
        "    # ADVERTENCIA: Procesar todo puede tardar MUCHO tiempo y consumir recursos.\n",
        "    num_noticias_a_procesar = 5\n",
        "    print(f\"Se procesarán las primeras {num_noticias_a_procesar} noticias.\")\n",
        "\n",
        "    for index, row in df_noticias.head(num_noticias_a_procesar).iterrows():\n",
        "        news_title = row['title']\n",
        "        news_text = row['body'] # Usar el cuerpo original para más contexto\n",
        "        news_id = f\"news_{index}\"\n",
        "\n",
        "        print(f\"\\n--- Procesando Noticia #{index}: {news_title[:60]}... ---\")\n",
        "\n",
        "        if not news_text or not isinstance(news_text, str):\n",
        "             print(\"  Advertencia: Cuerpo de la noticia vacío o inválido. Saltando.\")\n",
        "             structured_results.append({\n",
        "                 'indice_noticia': index, 'titulo': news_title,\n",
        "                 f'favor_{POLITICO_ANALISIS.split()[0]}': [\"Texto de noticia inválido\"],\n",
        "                 f'contra_{POLITICO_ANALISIS.split()[0]}': [\"Texto de noticia inválido\"],\n",
        "                 'aspectos_clave': [\"Texto de noticia inválido\"],\n",
        "                 'respuesta_llm_cruda': \"N/A - Texto inválido\"\n",
        "             })\n",
        "             continue\n",
        "\n",
        "        # Crear el prompt para la extracción estructurada en JSON\n",
        "        prompt_template = f\"\"\"\n",
        "        Analiza el siguiente texto de noticia con un enfoque específico en la figura política: '{POLITICO_ANALISIS}'.\n",
        "        Extrae la siguiente información y preséntala ESTRICTAMENTE en formato JSON. Asegúrate de que el JSON esté bien formado y sea el ÚNICO contenido de tu respuesta.\n",
        "\n",
        "        Formato JSON esperado:\n",
        "        {{\n",
        "          \"puntos_a_favor\": [\"lista de frases textuales del artículo que sean favorables a {POLITICO_ANALISIS}\"],\n",
        "          \"puntos_en_contra\": [\"lista de frases textuales del artículo que sean críticas o desfavorables a {POLITICO_ANALISIS}\"],\n",
        "          \"menciones_neutrales\": [\"lista de frases textuales donde se menciona a {POLITICO_ANALISIS} sin una clara connotación positiva o negativa\"],\n",
        "          \"resumen_general\": \"Un resumen muy breve (1-2 frases) del contenido principal de la noticia, independientemente de si menciona a {POLITICO_ANALISIS}.\"\n",
        "        }}\n",
        "\n",
        "        Instrucciones importantes:\n",
        "        - Basa tu análisis ÚNICAMENTE en el texto proporcionado. No añadas información externa ni opiniones.\n",
        "        - Si '{POLITICO_ANALISIS}' no es mencionado en absoluto, las listas 'puntos_a_favor', 'puntos_en_contra' y 'menciones_neutrales' deben ser listas vacías []. El resumen general aún debe proporcionarse.\n",
        "        - Si hay menciones pero no son claramente favorables o desfavorables, ponlas en 'menciones_neutrales'.\n",
        "        - Si no hay puntos claros a favor o en contra, devuelve listas vacías [] para esas claves.\n",
        "        - Tu respuesta DEBE ser SÓLO el objeto JSON, sin ningún texto introductorio o explicativo antes o después.\n",
        "\n",
        "        Texto de la Noticia:\n",
        "        --- START OF TEXT ---\n",
        "        {news_text}\n",
        "        --- END OF TEXT ---\n",
        "\n",
        "        JSON Output:\n",
        "        \"\"\"\n",
        "\n",
        "        # Llamar a Ollama\n",
        "        structured_data = {}\n",
        "        llm_raw_output = \"\"\n",
        "        try:\n",
        "            response = ollama.chat(\n",
        "                model=DEFAULT_OLLAMA_MODEL,\n",
        "                messages=[{'role': 'user', 'content': prompt_template}],\n",
        "                options={'temperature': 0.0}, # Temperatura cero para máxima consistencia\n",
        "                format=\"json\" # ¡Pedir formato JSON directamente a Ollama si la versión lo soporta!\n",
        "            )\n",
        "            llm_raw_output = response['message']['content']\n",
        "            print(f\"  Respuesta cruda del LLM recibida.\")# Primeros 100 chars: {llm_raw_output[:100]}...\")\n",
        "\n",
        "            # Intentar parsear la respuesta JSON del LLM\n",
        "            # ADVERTENCIA: Sin try-except robusto, esto fallará si la respuesta no es JSON válido.\n",
        "            # El format=\"json\" ayuda mucho pero no es 100% garantizado.\n",
        "            # Minimal check for JSON structure\n",
        "            json_string = llm_raw_output.strip()\n",
        "            if json_string.startswith('{') and json_string.endswith('}'):\n",
        "                 structured_data = json.loads(json_string)\n",
        "                 print(\"  JSON parseado con éxito.\")\n",
        "            else:\n",
        "                 print(\"  ADVERTENCIA: La respuesta del LLM no parece ser un JSON válido completo.\")\n",
        "                 print(f\"  Respuesta recibida: {llm_raw_output}\")\n",
        "                 # Intentar encontrar JSON dentro (menos fiable)\n",
        "                 json_start = llm_raw_output.find('{')\n",
        "                 json_end = llm_raw_output.rfind('}') + 1\n",
        "                 if json_start != -1 and json_end != -1:\n",
        "                     try:\n",
        "                         json_string_extracted = llm_raw_output[json_start:json_end]\n",
        "                         structured_data = json.loads(json_string_extracted)\n",
        "                         print(\"  JSON extraído y parseado con éxito desde la respuesta.\")\n",
        "                     except json.JSONDecodeError as e_extract:\n",
        "                         print(f\"  ERROR: No se pudo decodificar el JSON extraído. Error: {e_extract}\")\n",
        "                         structured_data = {\"error\": \"JSON Decode Error after extraction\", \"raw_output\": llm_raw_output}\n",
        "                 else:\n",
        "                    structured_data = {\"error\": \"Valid JSON object not found in response\", \"raw_output\": llm_raw_output}\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ERROR durante la llamada a Ollama o el parseo JSON: {e}\")\n",
        "            structured_data = {\"error\": str(e), \"raw_output\": llm_raw_output} # Guardar error\n",
        "\n",
        "        # Extraer la información usando .get() para manejar claves faltantes o errores\n",
        "        favor = structured_data.get('puntos_a_favor', [\"Error/No encontrado\"])\n",
        "        contra = structured_data.get('puntos_en_contra', [\"Error/No encontrado\"])\n",
        "        neutral = structured_data.get('menciones_neutrales', [\"Error/No encontrado\"])\n",
        "        resumen = structured_data.get('resumen_general', \"Error/No encontrado\")\n",
        "\n",
        "        # Guardar los resultados\n",
        "        structured_results.append({\n",
        "            'indice_noticia': index,\n",
        "            'titulo': news_title,\n",
        "            f'favor_{POLITICO_ANALISIS.split()[0]}': favor,\n",
        "            f'contra_{POLITICO_ANALISIS.split()[0]}': contra,\n",
        "            f'neutral_{POLITICO_ANALISIS.split()[0]}': neutral,\n",
        "            'resumen_noticia': resumen,\n",
        "            'respuesta_llm_cruda': llm_raw_output # Guardar para referencia/debug\n",
        "        })\n",
        "\n",
        "    # Crear DataFrame final con la información estructurada\n",
        "    print(\"\\n--- Creando DataFrame con información estructurada ---\")\n",
        "    df_structured_analysis = pd.DataFrame(structured_results)\n",
        "\n",
        "    # Mostrar el DataFrame resultante\n",
        "    pd.set_option('display.max_colwidth', 100) # Ajustar ancho de columna\n",
        "    pd.set_option('display.max_rows', 50) # Ajustar filas mostradas\n",
        "    print(df_structured_analysis)\n",
        "\n",
        "else:\n",
        "    print(\"--- No se puede realizar el análisis estructurado: 'df_noticias' no está disponible o no tiene las columnas 'title' y 'body'. ---\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"--- PROCESO COMPLETADO ---\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "H2yeUjIi989m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_structured_analysis"
      ],
      "metadata": {
        "id": "lEloeGOLFrNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kGB_ZdSZFrmE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}